{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Read the benign and dga data with low_memory=False\n",
        "benign = pd.read_csv(\"benign2lakh.csv\")\n",
        "benign['class'] = 'benign'\n",
        "\n",
        "dga = pd.read_csv(\"dga2lakh.csv\", low_memory=False)\n",
        "dga['class'] = 'dga'\n",
        "\n",
        "# Combine the dataframes\n",
        "data = pd.concat([benign, dga])\n",
        "\n",
        "# Set random seed\n",
        "seed = 1234\n",
        "\n",
        "# Update the 'matched_word' column\n",
        "data['matched_word'] = data['matched_word'].apply(lambda x: '1' if x != '' else '0')\n",
        "data['matched_word'] = data['matched_word'].astype('category')\n",
        "\n",
        "# Update the 'feedback_warning' column\n",
        "data['feedback_warning'] = data['feedback_warning'].apply(lambda x: '1' if pd.notna(x) and x != '' else '0')\n",
        "data['feedback_warning'] = data['feedback_warning'].astype('category')\n",
        "\n",
        "# Convert the 'class' column to a categorical variable\n",
        "data['class'] = data['class'].astype('category')\n",
        "\n",
        "# Remove the 4th column (index 3 in zero-based indexing)\n",
        "data = data.drop(data.columns[3], axis=1)\n",
        "\n",
        "# Train-Test Split\n",
        "X = data.drop(columns=['class', data.columns[0]])  # Drop target and first column\n",
        "y = data['class']\n",
        "\n",
        "# Handle NaN values by imputing with mean (you can choose a different strategy)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "# Scale the data using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=seed)\n",
        "\n",
        "# Create and train the Random Forest model without hyperparameter tuning\n",
        "clf_before_tuning = RandomForestClassifier(random_state=seed)\n",
        "clf_before_tuning.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set before tuning\n",
        "y_pred_before_tuning = clf_before_tuning.predict(X_test)\n",
        "\n",
        "# Confusion Matrix before tuning\n",
        "cm_before_tuning = confusion_matrix(y_test, y_pred_before_tuning)\n",
        "\n",
        "# Classification Report before tuning\n",
        "class_report_before_tuning = classification_report(y_test, y_pred_before_tuning, target_names=['benign', 'dga'])\n",
        "\n",
        "# Create and train the Random Forest model with hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "clf_after_tuning = RandomForestClassifier(random_state=seed)\n",
        "grid_search = GridSearchCV(estimator=clf_after_tuning, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set after tuning\n",
        "y_pred_after_tuning = best_rf.predict(X_test)\n",
        "\n",
        "# Confusion Matrix after tuning\n",
        "cm_after_tuning = confusion_matrix(y_test, y_pred_after_tuning)\n",
        "\n",
        "# Classification Report after tuning\n",
        "class_report_after_tuning = classification_report(y_test, y_pred_after_tuning, target_names=['benign', 'dga'])\n",
        "\n",
        "# Print results\n",
        "print(\"# Results Before Hyperparameter Tuning\")\n",
        "print(\"#\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_before_tuning)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report_before_tuning)\n",
        "print(\"#\")\n",
        "print(\"# Results After Hyperparameter Tuning\")\n",
        "print(\"#\")\n",
        "print(\"Best Parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm_after_tuning)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report_after_tuning)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqEwa8JOuboE",
        "outputId": "d640d012-40b7-4813-eb30-79433aaa0c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
            "# Results Before Hyperparameter Tuning\n",
            "#\n",
            "Confusion Matrix:\n",
            "[[78263  1738]\n",
            " [ 2527 77472]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.97      0.98      0.97     80001\n",
            "         dga       0.98      0.97      0.97     79999\n",
            "\n",
            "    accuracy                           0.97    160000\n",
            "   macro avg       0.97      0.97      0.97    160000\n",
            "weighted avg       0.97      0.97      0.97    160000\n",
            "\n",
            "#\n",
            "# Results After Hyperparameter Tuning\n",
            "#\n",
            "Best Parameters:\n",
            "{'criterion': 'entropy', 'max_depth': 20, 'max_features': 'log2', 'n_estimators': 100}\n",
            "Confusion Matrix:\n",
            "[[78584  1417]\n",
            " [ 2693 77306]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      benign       0.97      0.98      0.97     80001\n",
            "         dga       0.98      0.97      0.97     79999\n",
            "\n",
            "    accuracy                           0.97    160000\n",
            "   macro avg       0.97      0.97      0.97    160000\n",
            "weighted avg       0.97      0.97      0.97    160000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}